<div align="center">
  
# „ÄêNeurIPS'2025 Spotlightüî•„ÄëSeeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization
  
[![Conference](https://img.shields.io/badge/NeurIPS-2025(Spotlight)-FFD93D.svg)](https://neurips.cc/Conferences/2025)
[![Project](http://img.shields.io/badge/Project-SSHS-4D96FF.svg)](https://github.com/CuriseJia/SSHS/)
[![Paper](http://img.shields.io/badge/Paper-arxiv.2505.11217-FF6B6B.svg)](https://arxiv.org/abs/2505.11217)
</div>

The implementation of NeurIPS 2025 Spotlight (Top 3%) paper [Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization](https://arxiv.org/abs/2505.11217).

In this paper, we build the first framework to benchmark current Audio-Visual models' performance on Sound Localization task, a large scale stereo-image dataset generated by physics-based 3D simulator with Head-Related Transfer Function (HRTF) filtering and a neuroscience-inspired model, Cochleagram Audio-Visual Localization Model (CochAV) with Cochleagram representation.


## üì£ Updates
* **[2025/09/25]**: We are releasing the code in process. (Firstly the Unity rendering part, then the dataset part and finally the training and testing code. Due to some other DDLs, the total process may continue for a few weeks. Please understand.)
* **[2025/09/18]**: Our **SSHS** has been selected as a Spotlight paper at NeurIPS 2025! (Top 3% of 21575 submissions).


## üöÄ Quick Start
### Setup

#### Setup dataset generation environment
```shell
Unity 6000.10f1
MATLAB 2022b
```

#### Setup code environment
```shell
conda create -n SSHS python=3.9
conda activate SSHS
pip install -r requirements.txt
```

### Dataset Generation

Open the [UnityProject](https://github.com/CuriseJia/SSHS/tree/main/UnityProject) in the Unity, choose the [generation file](https://github.com/CuriseJia/SSHS/blob/main/UnityProject/Assets/SoundGenFinal.cs) and move to the console.

### Training
The training dataset contains about 1,800,000 pairs, and you can use Unity to generate and train. 
```shell
python train.py
```

### Test
The test dataset and the checkpoint can be downloaded here. 
```shell
python test.py
```

<div align=center>

|Datasets|Checkpoint|
|:--------:|:--------------:|
| [Download](https://drive.google.com/drive/folders/1gASarWGUdHcjQ0QAbrjIoekjDBMxMRcv?usp=sharing) | [Download](https://drive.google.com/drive/folders/1gASarWGUdHcjQ0QAbrjIoekjDBMxMRcv?usp=sharing) |

</div>


## üéóÔ∏è Acknowledgments
Our code is based on [IS3](https://github.com/kaistmm/SSLalignment) and [Pycochleagram](https://github.com/mcdermottLab/pycochleagram). We sincerely appreciate for their contributions.


## üìå Citation
If you find this paper useful, please consider staring üåü this repo and citing üìë our paper:
```
@article{jia2025seeing,
  title={Seeing sound, hearing sight: Uncovering modality bias and conflict of ai models in sound localization},
  author={Jia, Yanhao and Xie, Ji and Jivaganesh, S and Li, Hao and Wu, Xu and Zhang, Mengmi},
  journal={arXiv preprint arXiv:2505.11217},
  year={2025}
}
```

<details open><summary>üí° I also have other AI-Related projects that may interest you ‚ú®. </summary><p>

> [**Freestyleret: retrieving images from style-diversified queries**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03384.pdf)<br>
> Accepted by ECCV 2024 | [[Freestyleret Code]](https://github.com/CuriseJia/ECCV24-FreeStyleRet)<br>
> Hao Li*, Yanhao Jia*, Peng Jin, Zesen Cheng, Kehan Li, Jialu Sui, Chang Liu, Li Yuan

> [**Uni-Retrieval: A Multi-Style Retrieval Framework for STEM‚Äôs Education**](https://aclanthology.org/2025.acl-long.502/)<br>
> Accepted by ACL 2025 | [[Uni-Retrieval Code]](https://github.com/CuriseJia/ACL25-Uni-Retrieval)<br>
> Yanhao Jia, Xinyi Wu, Li Hao, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan