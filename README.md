<div align="center">
  
# 【NeurIPS'2025 Spotlight🔥】Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization
  
[![Conference](https://img.shields.io/badge/NeurIPS-2025(Spotlight)-FFD93D.svg)](https://neurips.cc/Conferences/2025)
[![Project](http://img.shields.io/badge/Project-SSHS-4D96FF.svg)](https://github.com/CuriseJia/SSHS/)
[![Paper](http://img.shields.io/badge/Paper-arxiv.2505.11217-FF6B6B.svg)](https://arxiv.org/abs/2505.11217)
</div>

The implementation of NeurIPS 2025 Spotlight (Top 3%) paper [Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization](https://arxiv.org/abs/2505.11217).

In this paper, we build the first framework to benchmark current Audio-Visual models' performance on Sound Localization task, a large scale stereo-image dataset generated by physics-based 3D simulator with Head-Related Transfer Function (HRTF) filtering and a neuroscience-inspired model, Cochleagram Audio-Visual Localization Model (CochAV) with Cochleagram representation.


## 📣 Updates
* **[2025/09/25]**: We are releasing the code in process. (Firstly the Unity rendering part, then the dataset part and finally the training and testing code. Due to some other DDLs, the total process may continue for a few weeks. Please understand.)
* **[2025/09/18]**: Our **SSHS** has been selected as a Spotlight paper at NeurIPS 2025! (Top 3% of 21575 submissions).


## 🚀 Quick Start
### Setup

#### Setup code environment
```shell
conda create -n SSHS python=3.9
conda activate SSHS
pip install -r requirements.txt
```


## 🎗️ Acknowledgments
Our code is based on [IS3](https://github.com/kaistmm/SSLalignment) and [Pycochleagram](https://github.com/mcdermottLab/pycochleagram). We sincerely appreciate for their contributions.


## 📌 Citation
If you find this paper useful, please consider staring 🌟 this repo and citing 📑 our paper:
```
@article{jia2025seeing,
  title={Seeing sound, hearing sight: Uncovering modality bias and conflict of ai models in sound localization},
  author={Jia, Yanhao and Xie, Ji and Jivaganesh, S and Li, Hao and Wu, Xu and Zhang, Mengmi},
  journal={arXiv preprint arXiv:2505.11217},
  year={2025}
}
```

<details open><summary>💡 I also have other AI-Related projects that may interest you ✨. </summary><p>

> [**Freestyleret: retrieving images from style-diversified queries**](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03384.pdf)<br>
> Accepted by ECCV 2024 | [[Freestyleret Code]](https://github.com/CuriseJia/ECCV24-FreeStyleRet)<br>
> Hao Li*, Yanhao Jia*, Peng Jin, Zesen Cheng, Kehan Li, Jialu Sui, Chang Liu, Li Yuan

> [**Uni-Retrieval: A Multi-Style Retrieval Framework for STEM’s Education**](https://aclanthology.org/2025.acl-long.502/)<br>
> Accepted by ACL 2025 | [[Uni-Retrieval Code]](https://github.com/CuriseJia/ACL25-Uni-Retrieval)<br>
> Yanhao Jia, Xinyi Wu, Li Hao, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan